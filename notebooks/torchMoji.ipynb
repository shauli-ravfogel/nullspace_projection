{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/torchMoji\n",
    "    \n",
    "from __future__ import print_function, division, unicode_literals\n",
    "# import example_helper\n",
    "import json\n",
    "import csv\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import emoji\n",
    "\n",
    "from torchmoji.sentence_tokenizer import SentenceTokenizer\n",
    "from torchmoji.model_def import torchmoji_emojis, torchmoji_feature_encoding\n",
    "from torchmoji.global_variables import PRETRAINED_PATH, VOCAB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoji map in emoji_overview.png\n",
    "EMOJIS = \":joy: :unamused: :weary: :sob: :heart_eyes: \\\n",
    ":pensive: :ok_hand: :blush: :heart: :smirk: \\\n",
    ":grin: :notes: :flushed: :100: :sleeping: \\\n",
    ":relieved: :relaxed: :raised_hands: :two_hearts: :expressionless: \\\n",
    ":sweat_smile: :pray: :confused: :kissing_heart: :heartbeat: \\\n",
    ":neutral_face: :information_desk_person: :disappointed: :see_no_evil: :tired_face: \\\n",
    ":v: :sunglasses: :rage: :thumbsup: :cry: \\\n",
    ":sleepy: :yum: :triumph: :hand: :mask: \\\n",
    ":clap: :eyes: :gun: :persevere: :smiling_imp: \\\n",
    ":sweat: :broken_heart: :yellow_heart: :musical_note: :speak_no_evil: \\\n",
    ":wink: :skull: :confounded: :smile: :stuck_out_tongue_winking_eye: \\\n",
    ":angry: :no_good: :muscle: :facepunch: :purple_heart: \\\n",
    ":sparkling_heart: :blue_heart: :grimacing: :sparkles:\".split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am so happy üòÑ ‚ò∫ üòä üòÅ üòç\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazary/anaconda3/envs/torchMoji/lib/python3.5/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "def top_elements(array, k):\n",
    "    ind = np.argpartition(array, -k)[-k:]\n",
    "    return ind[np.argsort(array[ind])][::-1]\n",
    "\n",
    "\n",
    "# Tokenizing using dictionary\n",
    "with open(VOCAB_PATH, 'r') as f:\n",
    "    vocabulary = json.load(f)\n",
    "\n",
    "st = SentenceTokenizer(vocabulary, 64)\n",
    "text = \"i am so happy\"\n",
    "# Loading model\n",
    "model = torchmoji_emojis(PRETRAINED_PATH)\n",
    "# Running predictions\n",
    "tokenized, _, _ = st.tokenize_sentences([text])\n",
    "# Get sentence probability\n",
    "prob = model(tokenized)[0]\n",
    "\n",
    "# Top emoji id\n",
    "emoji_ids = top_elements(prob, 5)\n",
    "\n",
    "# map to emojis\n",
    "emojis = map(lambda x: EMOJIS[x], emoji_ids)\n",
    "\n",
    "print(emoji.emojize(\"{} {}\".format(text,' '.join(emojis)), use_aliases=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "\n",
    "from torchmoji.lstm import LSTMHardSigmoid\n",
    "from torchmoji.attlayer import Attention\n",
    "from torchmoji.global_variables import NB_TOKENS, NB_EMOJI_CLASSES\n",
    "\n",
    "class CustomTorchMoji(nn.Module):\n",
    "    def __init__(self, nb_classes, nb_tokens, feature_output=False, output_logits=False,\n",
    "                 embed_dropout_rate=0, final_dropout_rate=0, return_attention=False):\n",
    "        \"\"\"\n",
    "        torchMoji model.\n",
    "        IMPORTANT: The model is loaded in evaluation mode by default (self.eval())\n",
    "        # Arguments:\n",
    "            nb_classes: Number of classes in the dataset.\n",
    "            nb_tokens: Number of tokens in the dataset (i.e. vocabulary size).\n",
    "            feature_output: If True the model returns the penultimate\n",
    "                            feature vector rather than Softmax probabilities\n",
    "                            (defaults to False).\n",
    "            output_logits:  If True the model returns logits rather than probabilities\n",
    "                            (defaults to False).\n",
    "            embed_dropout_rate: Dropout rate for the embedding layer.\n",
    "            final_dropout_rate: Dropout rate for the final Softmax layer.\n",
    "            return_attention: If True the model also returns attention weights over the sentence\n",
    "                              (defaults to False).\n",
    "        \"\"\"\n",
    "        super(CustomTorchMoji, self).__init__()\n",
    "\n",
    "        embedding_dim = 256\n",
    "        hidden_size = 512\n",
    "        attention_size = 4 * hidden_size + embedding_dim\n",
    "\n",
    "        self.feature_output = feature_output\n",
    "        self.embed_dropout_rate = embed_dropout_rate\n",
    "        self.final_dropout_rate = final_dropout_rate\n",
    "        self.return_attention = return_attention\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_logits = output_logits\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "        self.add_module('embed', nn.Embedding(nb_tokens, embedding_dim))\n",
    "        # dropout2D: embedding channels are dropped out instead of words\n",
    "        # many exampels in the datasets contain few words that losing one or more words can alter the emotions completely\n",
    "        self.add_module('embed_dropout', nn.Dropout2d(embed_dropout_rate))\n",
    "        self.add_module('lstm_0', LSTMHardSigmoid(embedding_dim, hidden_size, batch_first=True, bidirectional=True))\n",
    "        self.add_module('lstm_1', LSTMHardSigmoid(hidden_size*2, hidden_size, batch_first=True, bidirectional=True))\n",
    "        self.add_module('attention_layer', Attention(attention_size=attention_size, return_attention=return_attention))\n",
    "        if not feature_output:\n",
    "            self.add_module('final_dropout', nn.Dropout(final_dropout_rate))\n",
    "            if output_logits:\n",
    "                self.add_module('output_layer', nn.Sequential(nn.Linear(attention_size, nb_classes if self.nb_classes > 2 else 1)))\n",
    "            else:\n",
    "                self.add_module('output_layer', nn.Sequential(nn.Linear(attention_size, nb_classes if self.nb_classes > 2 else 1),\n",
    "                                                              nn.Softmax() if self.nb_classes > 2 else nn.Sigmoid()))\n",
    "        self.init_weights()\n",
    "        # Put model in evaluation mode by default\n",
    "        self.eval()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Here we reproduce Keras default initialization weights for consistency with Keras version\n",
    "        \"\"\"\n",
    "        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n",
    "        nn.init.uniform(self.embed.weight.data, a=-0.5, b=0.5)\n",
    "        for t in ih:\n",
    "            nn.init.xavier_uniform(t)\n",
    "        for t in hh:\n",
    "            nn.init.orthogonal(t)\n",
    "        for t in b:\n",
    "            nn.init.constant(t, 0)\n",
    "        if not self.feature_output:\n",
    "            nn.init.xavier_uniform(self.output_layer[0].weight.data)\n",
    "\n",
    "    def forward(self, input_seqs):\n",
    "        \"\"\" Forward pass.\n",
    "        # Arguments:\n",
    "            input_seqs: Can be one of Numpy array, Torch.LongTensor, Torch.Variable, Torch.PackedSequence.\n",
    "        # Return:\n",
    "            Same format as input format (except for PackedSequence returned as Variable).\n",
    "        \"\"\"\n",
    "        # Check if we have Torch.LongTensor inputs or not Torch.Variable (assume Numpy array in this case), take note to return same format\n",
    "        return_numpy = False\n",
    "        return_tensor = False\n",
    "        if isinstance(input_seqs, (torch.LongTensor, torch.cuda.LongTensor)):\n",
    "            input_seqs = Variable(input_seqs)\n",
    "            return_tensor = True\n",
    "        elif not isinstance(input_seqs, Variable):\n",
    "            input_seqs = Variable(torch.from_numpy(input_seqs.astype('int64')).long())\n",
    "            return_numpy = True\n",
    "\n",
    "        # If we don't have a packed inputs, let's pack it\n",
    "        reorder_output = False\n",
    "        if not isinstance(input_seqs, PackedSequence):\n",
    "            print('not packed')\n",
    "            ho = self.lstm_0.weight_hh_l0.data.new(2, input_seqs.size()[0], self.hidden_size).zero_()\n",
    "            co = self.lstm_0.weight_hh_l0.data.new(2, input_seqs.size()[0], self.hidden_size).zero_()\n",
    "\n",
    "            # Reorder batch by sequence length\n",
    "            input_lengths = torch.LongTensor([torch.max(input_seqs[i, :].data.nonzero()) + 1 for i in range(input_seqs.size()[0])])\n",
    "            input_lengths, perm_idx = input_lengths.sort(0, descending=True)\n",
    "            input_seqs = input_seqs[perm_idx][:, :input_lengths.max()]\n",
    "\n",
    "            # Pack sequence and work on data tensor to reduce embeddings/dropout computations\n",
    "            packed_input = pack_padded_sequence(input_seqs, input_lengths.cpu().numpy(), batch_first=True)\n",
    "            reorder_output = True\n",
    "        else:\n",
    "            ho = self.lstm_0.weight_hh_l0.data.data.new(2, input_seqs.size()[0], self.hidden_size).zero_()\n",
    "            co = self.lstm_0.weight_hh_l0.data.data.new(2, input_seqs.size()[0], self.hidden_size).zero_()\n",
    "            input_lengths = input_seqs.batch_sizes\n",
    "            packed_input = input_seqs\n",
    "\n",
    "        hidden = (Variable(ho, requires_grad=False), Variable(co, requires_grad=False))\n",
    "\n",
    "        # Embed with an activation function to bound the values of the embeddings\n",
    "        x = self.embed(packed_input.data)\n",
    "        x = nn.Tanh()(x)\n",
    "\n",
    "        # pyTorch 2D dropout2d operate on axis 1 which is fine for us\n",
    "        x = self.embed_dropout(x)\n",
    "\n",
    "        # Update packed sequence data for RNN\n",
    "        packed_input = PackedSequence(x, packed_input.batch_sizes)\n",
    "\n",
    "        # skip-connection from embedding to output eases gradient-flow and allows access to lower-level features\n",
    "        # ordering of the way the merge is done is important for consistency with the pretrained model\n",
    "        lstm_0_output, _ = self.lstm_0(packed_input, hidden)\n",
    "        lstm_1_output, _ = self.lstm_1(lstm_0_output, hidden)\n",
    "\n",
    "        # Update packed sequence data for attention layer\n",
    "        packed_input = PackedSequence(torch.cat((lstm_1_output.data,\n",
    "                                                 lstm_0_output.data,\n",
    "                                                 packed_input.data), dim=1),\n",
    "                                      packed_input.batch_sizes)\n",
    "\n",
    "        input_seqs, _ = pad_packed_sequence(packed_input, batch_first=True)\n",
    "\n",
    "        x, att_weights = self.attention_layer(input_seqs, input_lengths)\n",
    "\n",
    "        # output class probabilities or penultimate feature vector\n",
    "        feats_out = deepcopy(x.data.numpy())\n",
    "        if not self.feature_output:\n",
    "#             x = self.final_dropout(x)\n",
    "            outputs = self.output_layer(x)\n",
    "    \n",
    "        else:\n",
    "            outputs = x\n",
    "#         print(outputs)\n",
    "        # Reorder output if needed\n",
    "        if reorder_output:\n",
    "            reorered = Variable(outputs.data.new(outputs.size()))\n",
    "            reorered[perm_idx] = outputs\n",
    "            outputs = reorered\n",
    "#         print(outputs)\n",
    "        # Adapt return format if needed\n",
    "        if return_tensor:\n",
    "            outputs = outputs.data\n",
    "        if return_numpy:\n",
    "            outputs = outputs.data.numpy()\n",
    "\n",
    "        if self.return_attention:\n",
    "            return outputs, att_weights\n",
    "        else:\n",
    "            return outputs, feats_out\n",
    "        \n",
    "    def softmax_layer_output(self, x):\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTorchMoji(nb_classes=NB_EMOJI_CLASSES,\n",
    "                 nb_tokens=NB_TOKENS,\n",
    "                 return_attention=False)\n",
    "model.load_state_dict(torch.load(PRETRAINED_PATH))\n",
    "# return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not packed\n",
      "i am so happy üòÑ ‚ò∫ üòä üòÅ üòç\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazary/anaconda3/envs/torchMoji/lib/python3.5/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "prob, feats = model(tokenized)\n",
    "\n",
    "# Top emoji id\n",
    "emoji_ids = top_elements(prob[0], 5)\n",
    "\n",
    "# map to emojis\n",
    "emojis = map(lambda x: EMOJIS[x], emoji_ids)\n",
    "\n",
    "print(emoji.emojize(\"{} {}\".format(text,' '.join(emojis)), use_aliases=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lazary/anaconda3/envs/torchMoji/lib/python3.5/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "new_probs = model.softmax_layer_output(Variable(torch.from_numpy(feats))).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am so happy üòÑ ‚ò∫ üòä üòÅ üòç\n"
     ]
    }
   ],
   "source": [
    "emoji_ids = top_elements(new_probs[0], 5)\n",
    "\n",
    "# map to emojis\n",
    "emojis = map(lambda x: EMOJIS[x], emoji_ids)\n",
    "\n",
    "print(emoji.emojize(\"{} {}\".format(text,' '.join(emojis)), use_aliases=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchMoji",
   "language": "python",
   "name": "torchmoji"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
